"""
–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π BGE/Gemma
"""

import logging
import hashlib
import pickle
import os
from pathlib import Path
from typing import List, Union, Optional
import numpy as np

from sentence_transformers import SentenceTransformer
from local_config import EMBEDDING_MODEL

logger = logging.getLogger(__name__)

# –ü—ã—Ç–∞–µ–º—Å—è –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å FlagEmbedding
try:
    from FlagEmbedding import FlagModel
    FLAG_EMBEDDING_AVAILABLE = True
except ImportError:
    FLAG_EMBEDDING_AVAILABLE = False
    logger.warning("‚ö†Ô∏è FlagEmbedding –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ SentenceTransformer")


class EmbeddingCache:
    """–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–∞ –¥–∏—Å–∫"""

    def __init__(self, cache_dir: str = "embedding_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        logger.info(f"üíæ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {self.cache_dir}")

    def get_hash(self, text: str) -> str:
        """–•–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def get(self, text: str) -> Optional[np.ndarray]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∏–∑ –∫—ç—à–∞"""
        hash_id = self.get_hash(text)
        cache_file = self.cache_dir / f"{hash_id}.pkl"

        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    embedding = pickle.load(f)
                logger.debug(f"üíæ –ö—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏–µ: {hash_id[:8]}...")
                return embedding
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞ {hash_id}: {repr(e)}")
                return None
        return None

    def set(self, text: str, embedding: np.ndarray):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤ –∫—ç—à"""
        hash_id = self.get_hash(text)
        cache_file = self.cache_dir / f"{hash_id}.pkl"

        try:
            with open(cache_file, 'wb') as f:
                pickle.dump(embedding, f)
            logger.debug(f"üíæ –ö—ç—à —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {hash_id[:8]}...")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞ {hash_id}: {repr(e)}")


class EmbeddingManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π BGE/Gemma –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""

    def __init__(self, model_name: str = None):
        self.model_name = model_name or EMBEDDING_MODEL
        self.cache = EmbeddingCache()
        self.model = None
        self.is_flag_model = False
        self._init_model()
        logger.info(f"üî§ EmbeddingManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {self.model_name}")

    def _init_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å fallback"""
        try:
            model_lower = self.model_name.lower()

            # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å BGE —á–µ—Ä–µ–∑ FlagEmbedding –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            if "bge" in model_lower and FLAG_EMBEDDING_AVAILABLE:
                logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º BGE –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ FlagEmbedding: {self.model_name}")

                # –í—ã–±–∏—Ä–∞–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
                if "zh" in model_lower:
                    query_instruction = "‰∏∫Ëøô‰∏™Âè•Â≠êÁîüÊàêË°®Á§∫‰ª•Áî®‰∫éÊ£ÄÁ¥¢Áõ∏ÂÖ≥ÊñáÁ´†Ôºö"
                else:
                    query_instruction = "Represent this sentence for searching relevant passages:"

                self.model = FlagModel(
                    self.model_name,
                    query_instruction_for_retrieval=query_instruction,
                    use_fp16=False,  # –î–ª—è CPU
                )
                self.is_flag_model = True
                logger.info(f"‚úÖ BGE –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {self.model_name}")

            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º SentenceTransformer –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º SentenceTransformer: {self.model_name}")
                self.model = SentenceTransformer(self.model_name, device="cpu")
                self.is_flag_model = False
                logger.info(f"‚úÖ SentenceTransformer –∑–∞–≥—Ä—É–∂–µ–Ω: {self.model_name}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {self.model_name}: {repr(e)}")
            # Fallback –Ω–∞ MiniLM
            logger.info("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º fallback –º–æ–¥–µ–ª—å: all-MiniLM-L6-v2")
            self.model = SentenceTransformer("all-MiniLM-L6-v2", device="cpu")
            self.model_name = "all-MiniLM-L6-v2"
            self.is_flag_model = False

    def encode(self, texts: Union[str, List[str]], use_cache: bool = True, batch_size: int = 32, **kwargs) -> np.ndarray:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
        if isinstance(texts, str):
            texts = [texts]

        embeddings = []
        uncached_texts = []
        uncached_indices = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        if use_cache:
            for i, text in enumerate(texts):
                cached = self.cache.get(text)
                if cached is not None:
                    embeddings.append(cached)
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(i)
        else:
            uncached_texts = texts
            uncached_indices = list(range(len(texts)))

        # –ö–æ–¥–∏—Ä—É–µ–º –Ω–µ –∑–∞–∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        if uncached_texts:
            total_uncached = len(uncached_texts)
            logger.debug(f"üî§ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ {total_uncached} —Ç–µ–∫—Å—Ç–æ–≤...")

            if self.is_flag_model:
                # FlagModel (BGE)
                if hasattr(self.model, 'encode_queries'):
                    # –î–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤
                    new_embeddings = self.model.encode_queries(uncached_texts)
                else:
                    # –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
                    new_embeddings = self.model.encode(uncached_texts)
            else:
                # SentenceTransformer - –±–∞—Ç—á-–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
                new_embeddings = []
                for batch_start in range(0, len(uncached_texts), batch_size):
                    batch_end = min(batch_start + batch_size, len(uncached_texts))
                    batch_texts = uncached_texts[batch_start:batch_end]

                    # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
                    percent = (batch_end / len(uncached_texts)) * 100
                    logger.debug(f"üî§ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞—Ç—á–∞: {batch_start+1}-{batch_end}/{len(uncached_texts)} ({percent:.1f}%)")

                    batch_embeddings = self.model.encode(
                        batch_texts,
                        normalize_embeddings=True,
                        **kwargs
                    )
                    new_embeddings.extend(batch_embeddings)

                new_embeddings = np.array(new_embeddings)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            for i, (text, emb) in enumerate(zip(uncached_texts, new_embeddings)):
                self.cache.set(text, emb)

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            if use_cache:
                # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫
                all_embeddings = [None] * len(texts)
                for i, emb in enumerate(embeddings):
                    all_embeddings[i] = emb

                for idx, emb in zip(uncached_indices, new_embeddings):
                    all_embeddings[idx] = emb

                return np.array(all_embeddings)
            else:
                return new_embeddings

        return np.array(embeddings)

    def get_embedding_dimension(self) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        # –ü—Ä–æ–±—É–µ–º –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        try:
            if hasattr(self.model, 'get_sentence_embedding_dimension'):
                return self.model.get_sentence_embedding_dimension()

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏
            model_lower = self.model_name.lower()

            if "m3" in model_lower or "large" in model_lower:
                return 1024
            elif "base" in model_lower:
                return 768
            elif "mini" in model_lower:
                return 384
            elif "e5-large" in model_lower:
                return 1024
            elif "e5-base" in model_lower:
                return 768
            else:
                # –¢–µ—Å—Ç–æ–≤–∞—è –∫–æ–¥–∏—Ä–æ–≤–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
                test_emb = self.encode("test", use_cache=False)
                return test_emb.shape[1]

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º 384: {repr(e)}")
            return 384

    def clear_cache(self):
        """–û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        cache_files = list(self.cache.cache_dir.glob("*.pkl"))
        for file in cache_files:
            file.unlink()
        logger.info(f"üóëÔ∏è –û—á–∏—â–µ–Ω –∫—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {len(cache_files)} —Ñ–∞–π–ª–æ–≤")