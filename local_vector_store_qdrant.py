# local_vector_store_qdrant.py
import os
import hashlib
import logging
from typing import List, Dict, Any, Optional, Callable

import requests
import numpy as np
from sentence_transformers import SentenceTransformer

from local_config import EMBEDDING_MODEL_NAME, QDRANT_URL, QDRANT_COLLECTION

logger = logging.getLogger(__name__)
CACHE_DIR = "embedding_cache"
os.makedirs(CACHE_DIR, exist_ok=True)


class LocalQdrantVectorStore:
    def __init__(self, model_name: Optional[str] = None):
        # Ð±ÐµÑ€Ñ‘Ð¼ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð¸Ð· local_config, Ð½Ð¾ Ð´Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼ override Ñ‡ÐµÑ€ÐµÐ· Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚
        self.model_name = model_name or EMBEDDING_MODEL_NAME
        # ÑÐ²Ð½Ñ‹Ð¹ fallback Ð½Ð° MiniLM â€” Ð±ÑƒÐ´ÐµÑ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½ Ð¿Ñ€Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐµ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ñ‚ÑÐ¶ÐµÐ»Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        self.fallback_model = "all-MiniLM-L6-v2"

        logger.info("ðŸ”¤ Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ð¾Ð³Ð¾ ÑÑ‚Ð¾Ñ€Ð°. Ð—Ð°Ð¿Ñ€Ð¾ÑˆÐµÐ½Ð½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ: %s", self.model_name)
        self.emb_model = self._load_embedding_model_with_fallback(self.model_name, self.fallback_model)

        self.qdrant_url = QDRANT_URL.rstrip("/")
        self.collection = QDRANT_COLLECTION

        self._ensure_collection()

    def _load_embedding_model_with_fallback(self, preferred: str, fallback: str) -> SentenceTransformer:
        """
        ÐŸÑ‹Ñ‚Ð°ÐµÑ‚ÑÑ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ preferred Ð¼Ð¾Ð´ÐµÐ»ÑŒ. ÐŸÑ€Ð¸ Ð»ÑŽÐ±Ð¾Ð¹ Ð¾ÑˆÐ¸Ð±ÐºÐµ â€” Ð»Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ Ð¸ Ð·Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ fallback.
        Ð­Ñ‚Ð¾ Ð´ÐµÐ»Ð°ÐµÑ‚ Ñ€Ð°Ð±Ð¾Ñ‚Ñƒ ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾Ð¹ Ð½Ð° CPU Ð¸ Ð±ÐµÐ· Ð³ÐµÐ¼Ð¾Ñ€Ñ€Ð¾Ñ Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒÑŽ.
        """
        try:
            # Ð•ÑÐ»Ð¸ preferred ÑÐ²Ð½Ð¾ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ñ‚ "bge" â€” Ð¿Ñ€Ð¸Ð½ÑƒÐ´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð½Ðµ Ð³Ñ€ÑƒÐ·Ð¸Ð¼ Ð½Ð° Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½ÑƒÑŽ Ð¼Ð°ÑˆÐ¸Ð½Ñƒ,
            # Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð¿Ñ‹Ñ‚Ð°Ñ‚ÑŒÑÑ Ñ‚Ð°Ñ‰Ð¸Ñ‚ÑŒ Ñ‚ÑÐ¶Ñ‘Ð»ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð² CPU ÑÑ€ÐµÐ´Ñƒ.
            if preferred and "bge" in preferred.lower():
                raise RuntimeError("Preferred model looks like BGE/Gemma2 â€” skipping heavy local load.")

            logger.info("ðŸ”„ ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÑŒ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²: %s", preferred)
            model = SentenceTransformer(preferred, device="cpu", trust_remote_code=True)
            logger.info("âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð° Ð¼Ð¾Ð´ÐµÐ»ÑŒ: %s", preferred)
            return model
        except Exception as e:
            logger.warning("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð¾ÑÐ½Ð¾Ð²Ð½ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ '%s': %s", preferred, repr(e))
            logger.info("ðŸ” ÐŸÐµÑ€ÐµÐºÐ»ÑŽÑ‡Ð°ÐµÐ¼ÑÑ Ð½Ð° fallback-ÑÐ¼Ð±ÐµÐ´Ð´ÐµÑ€: %s", fallback)
            try:
                model = SentenceTransformer(fallback, device="cpu")
                logger.info("âœ… Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½ fallback: %s", fallback)
                # ÐžÐ±Ð½Ð¾Ð²Ð¸Ð¼ Ð¸Ð¼Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð±Ñ‹Ð» Ð¿Ð¾Ð½ÑÑ‚Ð½Ñ‹Ð¹ Ð»Ð¾Ð³/Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ†Ð¸Ñ
                self.model_name = fallback
                return model
            except Exception as e2:
                logger.error("âŒ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ fallback-ÑÐ¼Ð±ÐµÐ´Ð´ÐµÑ€ '%s': %s", fallback, repr(e2))
                raise RuntimeError("Failed to load embedding model and fallback") from e2

    def _ensure_collection(self):
        url = f"{self.qdrant_url}/collections/{self.collection}"
        try:
            resp = requests.get(url, timeout=5)
            if resp.status_code == 200:
                logger.info("ðŸ“š ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ Qdrant '%s' ÑƒÐ¶Ðµ ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÐµÑ‚", self.collection)
                return
        except Exception:
            # ÐµÑÐ»Ð¸ Ð¾ÑˆÐ¸Ð±ÐºÐ° â€” Ð±ÑƒÐ´ÐµÐ¼ Ð¿Ñ‹Ñ‚Ð°Ñ‚ÑŒÑÑ ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ
            pass

        logger.info("ðŸ“š Ð¡Ð¾Ð·Ð´Ð°ÑŽ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ Qdrant '%s'...", self.collection)
        dim = self.emb_model.get_sentence_embedding_dimension()
        payload = {
            "vectors": {"size": dim, "distance": "Cosine"},
        }
        resp = requests.put(url, json=payload, timeout=10)
        resp.raise_for_status()
        logger.info("âœ… ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ '%s' ÑÐ¾Ð·Ð´Ð°Ð½Ð° (Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ: %d)", self.collection, dim)

    def clear_collection(self, progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None):
        """ÐŸÐ¾Ð»Ð½Ð¾Ðµ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ð¸ (Ð´Ð»Ñ Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¿ÐµÑ€ÐµÐ¸Ð½Ð´ÐµÐºÑÐ°Ñ†Ð¸Ð¸)."""
        if progress_callback:
            progress_callback({"stage": "clearing", "done": 0, "total": 1, "message": "ÐžÑ‡Ð¸Ñ‰Ð°ÐµÐ¼ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ Qdrant..."})
        url = f"{self.qdrant_url}/collections/{self.collection}/points/delete"
        try:
            resp = requests.post(url, json={"filter": {}}, timeout=60)
            resp.raise_for_status()
            if progress_callback:
                progress_callback({"stage": "clearing", "done": 1, "total": 1, "message": "ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð°."})
            logger.info("ðŸ—‘ ÐšÐ¾Ð»Ð»ÐµÐºÑ†Ð¸Ñ Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð°")
        except Exception as e:
            logger.warning("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¾Ñ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ: %s", e)
            # Ð½Ðµ Ñ„ÐµÐ¹Ð»Ð¸Ð¼, Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð¸Ð¼

    def _hash_text(self, text: str) -> str:
        return hashlib.md5(text.encode("utf-8")).hexdigest()

    def _get_cached_embedding(self, text: str) -> Optional[np.ndarray]:
        h = self._hash_text(text)
        path = os.path.join(CACHE_DIR, f"{h}.npy")
        if os.path.exists(path):
            try:
                return np.load(path)
            except Exception:
                return None
        return None

    def _save_cached_embedding(self, text: str, emb: np.ndarray):
        h = self._hash_text(text)
        path = os.path.join(CACHE_DIR, f"{h}.npy")
        try:
            np.save(path, emb)
        except Exception as e:
            logger.warning("âš ï¸ ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ ÑÐ¾Ñ…Ñ€Ð°Ð½Ð¸Ñ‚ÑŒ ÐºÑÑˆ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°: %s", e)

    def _encode_batch(self, texts: List[str]) -> np.ndarray:
        """Batch-encode with sentence-transformers (returns numpy array)."""
        embs = self.emb_model.encode(
            texts,
            show_progress_bar=False,
            convert_to_numpy=True,
            normalize_embeddings=True,
            batch_size=32,
        )
        return embs

    def rebuild_collection(self, docs: List[Dict[str, Any]], progress_callback: Optional[Callable[[Dict[str, Any]], None]] = None):
        """
        Ð—Ð°Ð³Ñ€ÑƒÐ¶Ð°ÐµÑ‚ Ð²ÐµÐºÑ‚Ð¾Ñ€Ñ‹ Ð² ÐºÐ¾Ð»Ð»ÐµÐºÑ†Ð¸ÑŽ (Ð±ÐµÐ· ÑƒÐ´Ð°Ð»ÐµÐ½Ð¸Ñ ÑÑ‚Ð°Ñ€Ñ‹Ñ… Ñ‚Ð¾Ñ‡ÐµÐº).
        ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ progress_callback(progress_dict).
        progress_dict: {stage, done, total, message}
        """
        total_docs = len(docs)
        if total_docs == 0:
            logger.warning("âš ï¸ rebuild_collection: Ð¿ÑƒÑÑ‚Ð¾Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð²")
            return

        texts = [d["text"] for d in docs]

        # 1) ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÑÑˆ
        vectors = [None] * total_docs
        uncached_texts = []
        uncached_indices = []
        for i, t in enumerate(texts):
            cached = self._get_cached_embedding(t)
            if cached is not None:
                vectors[i] = cached
            else:
                uncached_texts.append(t)
                uncached_indices.append(i)

        if progress_callback:
            progress_callback({"stage": "cache_check", "done": total_docs - len(uncached_texts), "total": total_docs, "message": "ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÑÑˆÐ° ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²..."})

        # 2) ÐšÐ¾Ð´Ð¸Ñ€ÑƒÐµÐ¼ uncached Ð¿Ð°Ñ€Ñ‚Ð¸ÑÐ¼Ð¸
        if uncached_texts:
            batch_enc = 32
            for start in range(0, len(uncached_texts), batch_enc):
                batch = uncached_texts[start:start + batch_enc]
                embs = self._encode_batch(batch)
                for j, emb in enumerate(embs):
                    idx = uncached_indices[start + j]
                    vectors[idx] = emb
                    self._save_cached_embedding(batch[j], emb)
                if progress_callback:
                    progress_callback({"stage": "encoding", "done": min(len(uncached_texts), start + batch_enc), "total": len(uncached_texts), "message": "Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¾Ð²..."})
        else:
            if progress_callback:
                progress_callback({"stage": "encoding", "done": 0, "total": 0, "message": "Ð’ÑÐµ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹ Ð² ÐºÑÑˆÐµ."})

        # 3) ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ñ‚Ð¾Ñ‡ÐµÐº
        points = []
        for i, doc in enumerate(docs):
            vec = vectors[i]
            if vec is None:
                logger.warning("âš ï¸ Ð’ÐµÐºÑ‚Ð¾Ñ€ Ð´Ð»Ñ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð° %d Ð¿ÑƒÑÑ‚Ð¾Ð¹ â€” Ð¿Ñ€Ð¾Ð¿ÑƒÑÐºÐ°ÐµÐ¼", i)
                continue
            # ensure python floats
            vlist = np.array(vec).astype(float).tolist()
            points.append({
                "id": doc.get("id", i),
                "vector": vlist,
                "payload": {
                    "file": doc.get("file", ""),
                    "page": doc.get("page", 1),
                    "type": doc.get("type", "text"),
                    "text": doc.get("text", ""),
                },
            })

        # 4) Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð¿Ð°Ñ‡ÐºÐ°Ð¼Ð¸
        upload_batch = 256
        url = f"{self.qdrant_url}/collections/{self.collection}/points?wait=true"
        for start in range(0, len(points), upload_batch):
            batch = points[start:start + upload_batch]
            try:
                resp = requests.post(url, json={"points": batch}, timeout=120)
                resp.raise_for_status()
            except Exception as e:
                logger.exception("âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð±Ð°Ñ‚Ñ‡Ð° Ð² Qdrant: %s", e)
                # Ð¿Ñ€Ð¾Ð´Ð¾Ð»Ð¶Ð°ÐµÐ¼ Ð¿Ð¾Ð¿Ñ‹Ñ‚ÐºÐ¸ Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… Ð±Ð°Ñ‚Ñ‡ÐµÐ¹
                continue
            if progress_callback:
                progress_callback({"stage": "upload", "done": min(len(points), start + upload_batch), "total": len(points), "message": "Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð² Qdrant..."})

        if progress_callback:
            progress_callback({"stage": "done", "done": len(points), "total": len(points), "message": "Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°."})

        logger.info("âœ… Ð’ Qdrant Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ %d Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² (Ð¿Ñ€Ð¸Ð±Ð».)", len(points))

    def search(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:
        vector = self._encode_batch([query])[0].tolist()
        url = f"{self.qdrant_url}/collections/{self.collection}/points/search"
        payload = {"vector": vector, "limit": top_k, "with_payload": True, "with_vector": False}
        resp = requests.post(url, json=payload, timeout=30)
        resp.raise_for_status()
        data = resp.json()
        result = []
        for point in data.get("result", []):
            payload_data = point.get("payload", {}) or {}
            score = point.get("score", 0.0)
            result.append({
                "text": payload_data.get("text", ""),
                "file": payload_data.get("file", ""),
                "page": payload_data.get("page", 1),
                "type": payload_data.get("type", "text"),
                "score": score,
            })
        return result

    def stats(self) -> Dict[str, Any]:
        url = f"{self.qdrant_url}/collections/{self.collection}"
        try:
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
            data = resp.json()
            points_count = data.get("result", {}).get("points_count", 0)
        except Exception:
            points_count = 0
        return {"total_documents": points_count}
#