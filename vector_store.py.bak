import logging
import pickle
import json
from pathlib import Path
from typing import List, Dict
from collections import Counter

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

from config import INDEX_FILE, EMBEDDING_MODEL

logger = logging.getLogger(__name__)


class VectorStore:
    """–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å FAISS"""

    def __init__(self):
        self.model = SentenceTransformer(EMBEDDING_MODEL)
        self.dimension = self.model.get_sentence_embedding_dimension()
        self.index = faiss.IndexFlatL2(self.dimension)
        self.documents = []
        self.progress_file = Path("indexed_files.json")
        logger.info("‚úÖ VectorStore –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (—Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: %d)", self.dimension)

    def add_documents(self, docs: List[Dict]):
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –∏–Ω–¥–µ–∫—Å"""
        if not docs:
            return

        texts = [doc['content'] for doc in docs]
        embeddings = self.model.encode(texts, show_progress_bar=False)

        if embeddings.ndim == 1:
            embeddings = embeddings.reshape(1, -1)

        self.index.add(embeddings.astype('float32'))
        self.documents.extend(docs)

        logger.debug("–î–æ–±–∞–≤–ª–µ–Ω–æ %d –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å", len(docs))

    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É"""
        if self.index.ntotal == 0:
            logger.warning("–ò–Ω–¥–µ–∫—Å –ø—É—Å—Ç")
            return []

        query_embedding = self.model.encode([query])
        distances, indices = self.index.search(query_embedding.astype('float32'), top_k)

        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(self.documents):
                doc = self.documents[idx].copy()
                doc['score'] = float(1 / (1 + dist))
                results.append(doc)

        return results

    def save(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞"""
        try:
            faiss.write_index(self.index, str(INDEX_FILE))
            with open(str(INDEX_FILE) + '.docs', 'wb') as f:
                pickle.dump(self.documents, f)
            logger.info("üíæ –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: %s", INDEX_FILE)
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: %s", repr(e))

    def load(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞"""
        try:
            if not INDEX_FILE.exists():
                logger.warning("–§–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: %s", INDEX_FILE)
                return False

            self.index = faiss.read_index(str(INDEX_FILE))
            with open(str(INDEX_FILE) + '.docs', 'rb') as f:
                self.documents = pickle.load(f)

            logger.info("‚úÖ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: %d –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤", len(self.documents))
            return True
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞: %s", repr(e))
            return False

    def clear(self):
        """–ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∏–Ω–¥–µ–∫—Å–∞"""
        self.index = faiss.IndexFlatL2(self.dimension)
        self.documents = []
        logger.info("üóëÔ∏è –ò–Ω–¥–µ–∫—Å –æ—á–∏—â–µ–Ω")

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
        files = list(set(doc.get('file', 'Unknown') for doc in self.documents))
        types = Counter(doc.get('type', 'unknown') for doc in self.documents)

        return {
            'total_documents': len(self.documents),
            'total_files': len(files),
            'files': sorted(files),
            'types': dict(types),
        }

    def save_progress(self, indexed_files: List[str]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump({'indexed_files': indexed_files}, f, ensure_ascii=False, indent=2)
            logger.info("üíæ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω—ë–Ω: %d —Ñ–∞–π–ª–æ–≤", len(indexed_files))
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: %s", repr(e))

    def load_progress(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            if not self.progress_file.exists():
                return []

            with open(self.progress_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                indexed_files = data.get('indexed_files', [])
                logger.info("‚úÖ –ü—Ä–æ–≥—Ä–µ—Å—Å –∑–∞–≥—Ä—É–∂–µ–Ω: %d —Ñ–∞–π–ª–æ–≤", len(indexed_files))
                return indexed_files
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: %s", repr(e))
            return []

    def clear_progress(self):
        """–£–¥–∞–ª—è–µ—Ç —Ñ–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞"""
        try:
            if self.progress_file.exists():
                self.progress_file.unlink()
                logger.info("üóëÔ∏è –§–∞–π–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —É–¥–∞–ª—ë–Ω")
        except Exception as e:
            logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: %s", repr(e))