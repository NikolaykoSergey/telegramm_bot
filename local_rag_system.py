import logging
import json
import time
import psutil
from pathlib import Path
from typing import List, Dict, Optional

from tqdm import tqdm

from local_config import (
    DOCUMENTS_FOLDER,
    TOP_K_RESULTS,
)
from local_document_processor import DocumentProcessor
from local_vector_store import VectorStore
from local_ollama_client import OllamaClient

logger = logging.getLogger(__name__)


class RAGSystem:
    """RAG —Å–∏—Å—Ç–µ–º–∞: –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è + –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤"""

    def __init__(self):
        self.document_processor = DocumentProcessor()
        self.vector_store = VectorStore()
        self.ollama = OllamaClient()

        self.indexed_files_path = Path("indexed_files.json")
        self.indexed_files = self._load_indexed_files()

        self._indexing = False
        self._stop_indexing = False

        logger.info("‚úÖ RAGSystem –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def _load_indexed_files(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        if self.indexed_files_path.exists():
            try:
                with open(self.indexed_files_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    return data.get("indexed_files", [])
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ indexed_files.json: {repr(e)}")
        return []

    def _save_indexed_files(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            with open(self.indexed_files_path, 'w', encoding='utf-8') as f:
                json.dump({"indexed_files": self.indexed_files}, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è indexed_files.json: {repr(e)}")

    def index_documents(self, continue_indexing: bool = True):
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ documents/
        """
        import psutil
        import time

        if self._indexing:
            logger.warning("‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è")
            return

        self._indexing = True
        self._stop_indexing = False

        process_start = time.time()
        process = psutil.Process()

        try:
            if not continue_indexing:
                logger.info("üîÑ –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è (–æ—á–∏—Å—Ç–∫–∞ –ë–î)...")
                self.vector_store.clear_collection()
                self.indexed_files = []

            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
            files = list(DOCUMENTS_FOLDER.glob("*.pdf")) + list(DOCUMENTS_FOLDER.glob("*.docx"))

            if not files:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ {DOCUMENTS_FOLDER}")
                return

            # –§–∏–ª—å—Ç—Ä—É–µ–º —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ
            if continue_indexing:
                files = [f for f in files if f.name not in self.indexed_files]

            if not files:
                logger.info("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã")
                return

            logger.info(f"üìö –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {len(files)}")
            logger.info(f"üìä –ù–∞—á–∏–Ω–∞—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é (continue={continue_indexing})")
            logger.info(f"üíæ –ü–∞–º—è—Ç—å –≤ –Ω–∞—á–∞–ª–µ: {process.memory_info().rss / 1024 / 1024:.1f}MB")

            total_fragments = 0
            total_files_processed = 0

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª—ã
            for file_idx, file_path in enumerate(tqdm(files, desc="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è"), 1):
                if self._stop_indexing:
                    logger.info("üõë –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                    break

                logger.info(f"\nüìÅ –§–∞–π–ª {file_idx}/{len(files)}: {file_path.name}")
                file_start = time.time()

                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                fragments = self.document_processor.process_file(file_path)

                if not fragments:
                    logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {file_path.name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞")
                    continue

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
                logger.info(f"   üì§ –ó–∞–≥—Ä—É–∑–∫–∞ {len(fragments)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î...")
                db_start = time.time()
                self.vector_store.add_documents(fragments)
                db_time = time.time() - db_start

                # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π
                self.indexed_files.append(file_path.name)
                self._save_indexed_files()

                total_fragments += len(fragments)
                total_files_processed += 1

                file_time = time.time() - file_start
                memory_usage = process.memory_info().rss / 1024 / 1024

                logger.info(f"‚úÖ {file_path.name}:")
                logger.info(f"   üìä {len(fragments)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
                logger.info(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {file_time:.1f}—Å (–ë–î: {db_time:.1f}—Å)")
                logger.info(f"   üíæ –ü–∞–º—è—Ç—å: {memory_usage:.1f}MB")
                logger.info(f"   üìà –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {len(fragments) / file_time:.1f} —Ñ—Ä–∞–≥–º/—Å–µ–∫")

            total_time = time.time() - process_start
            final_memory = process.memory_info().rss / 1024 / 1024

            logger.info(f"\nüéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
            logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
            logger.info(f"   üìÅ –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_files_processed}/{len(files)}")
            logger.info(f"   üìÑ –í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {total_fragments}")
            logger.info(f"   ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}—Å")
            logger.info(f"   üìà –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_fragments / total_time:.1f} —Ñ—Ä–∞–≥–º/—Å–µ–∫")
            logger.info(f"   üíæ –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ: {final_memory:.1f}MB")

            if total_files_processed > 0:
                logger.info(f"   üìä –°—Ä–µ–¥–Ω–µ–µ –Ω–∞ —Ñ–∞–π–ª: {total_fragments / total_files_processed:.1f} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {repr(e)}")
            raise

        finally:
            self._indexing = False
            self._stop_indexing = False

    def is_indexing(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞: –∏–¥—ë—Ç –ª–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è"""
        return self._indexing

    def stop_indexing_process(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏"""
        self._stop_indexing = True

    def query(self, user_query: str, top_k: int = TOP_K_RESULTS) -> Dict:
        """
        –ü–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞

        Args:
            user_query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤

        Returns:
            {"answer": "–æ—Ç–≤–µ—Ç", "sources": [...], "relevance": float}
        """
        logger.info(f"üí¨ –ó–∞–ø—Ä–æ—Å: {user_query}")

        # 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        documents = self.vector_store.search(user_query, top_k=top_k)

        if not documents:
            return {
                "answer": "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.",
                "sources": [],
                "relevance": 0.0
            }

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        avg_score = sum(doc.get('score', 0) for doc in documents) / len(documents)
        relevance_percent = avg_score * 100

        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context_parts = []
        for idx, doc in enumerate(documents, start=1):
            context_parts.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {idx}: {doc['file']}, —Å—Ç—Ä. {doc['page']}]\n{doc['content']}"
            )

        context = "\n\n---\n\n".join(context_parts)

        # 3. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
        system_prompt = """–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π –ø–æ –ª–∏—Ñ—Ç–∞–º –∏ –ª–∏—Ñ—Ç–æ–≤–æ–º—É –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—é.

        –¢–í–û–ò –ñ–Å–°–¢–ö–ò–ï –ü–†–ê–í–ò–õ–ê:
        - –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        - –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏–ª–∏ —Å–≤–æ–∏ –¥–æ–≥–∞–¥–∫–∏.
        - –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ù–ï–¢ ‚Äî –ø—Ä—è–º–æ —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏: 
          "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ—Ç —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."
        - –ù–ï –ø–æ–¥–º–µ–Ω—è–π –æ–¥–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ/–ø–ª–∞—Ç—É –¥—Ä—É–≥–∏–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, –°–£–ö-1 != –ú–°-1). 
          –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç —Å–ª–æ–≤–∞ "–°–£–ö-1", –Ω–µ –≥–æ–≤–æ—Ä–∏, —á—Ç–æ "—Ä–µ—á—å –∏–¥—ë—Ç –æ –°–£–ö-1" –∏–ª–∏ "–°–£–ö-1 –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è". 
          –ü—Ä–æ—Å—Ç–æ —Å–∫–∞–∂–∏, —á—Ç–æ –ø–æ "–°–£–ö-1" –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç.
        - –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É, –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
        - –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–±–ª–∏—Ü–∞ –∏–ª–∏ —è–≤–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ (–ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª–∏, –∞–¥—Ä–µ—Å–∞, DIP, –ø–∞—Ä–∞–º–µ—Ç—Ä—ã) ‚Äî –ø—Ä–∏–≤–µ–¥–∏ –∏—Ö —Å–ª–æ–≤–∞–º–∏ –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ.

        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
        - –ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô –∏ –ù–ï –î–û–ì–ê–î–´–í–ê–ô–°–Ø.
        - –õ—É—á—à–µ —á–µ—Å—Ç–Ω–æ –æ—Ç–≤–µ—Ç—å "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ <—Ç–µ—Ä–º–∏–Ω> –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞", —á–µ–º –Ω–∞–ø–∏—Å–∞—Ç—å —á—É—à—å."""

        prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:

{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{user_query}

–û—Ç–≤–µ—Ç:"""

        answer = self.ollama.generate(prompt, system_prompt=system_prompt)

        # 4. –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = [
            {
                "file": doc["file"],
                "page": doc["page"],
                "score": round(doc["score"], 3),
            }
            for doc in documents
        ]

        return {
            "answer": answer,
            "sources": sources,
            "relevance": relevance_percent
        }

    def query_with_history(self, history: List[Dict], user_query: str, top_k: int = TOP_K_RESULTS) -> Dict:
        """
        –ü–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞

        Args:
            history: [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}, ...]
            user_query: –¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤

        Returns:
            {"answer": "–æ—Ç–≤–µ—Ç", "sources": [...], "relevance": float}
        """
        logger.info(f"üí¨ –ó–∞–ø—Ä–æ—Å —Å –∏—Å—Ç–æ—Ä–∏–µ–π: {user_query}")

        # 1. –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        documents = self.vector_store.search(user_query, top_k=top_k)

        if not documents:
            return {
                "answer": "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.",
                "sources": [],
                "relevance": 0.0
            }

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        avg_score = sum(doc.get('score', 0) for doc in documents) / len(documents)
        relevance_percent = avg_score * 100

        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        context_parts = []
        for idx, doc in enumerate(documents, start=1):
            context_parts.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {idx}: {doc['file']}, —Å—Ç—Ä. {doc['page']}]\n{doc['content']}"
            )

        context = "\n\n---\n\n".join(context_parts)

        # 3. –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é –¥–∏–∞–ª–æ–≥–∞
        history_text = ""
        if history:
            history_lines = []
            for msg in history[-10:]:  # –ë–µ—Ä—ë–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
                role = msg.get("role", "user")
                content = msg.get("content", "").strip()
                if content:
                    prefix = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:" if role == "user" else "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
                    history_lines.append(f"{prefix} {content}")

            if history_lines:
                history_text = "\n".join(history_lines)

        # 4. –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ LLM
        system_prompt = """–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π –ø–æ –ª–∏—Ñ—Ç–∞–º –∏ –ª–∏—Ñ—Ç–æ–≤–æ–º—É –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—é.

        –¢–í–û–ò –ñ–Å–°–¢–ö–ò–ï –ü–†–ê–í–ò–õ–ê:
        - –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
        - –ù–ò–ö–û–ì–î–ê –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è, –∏–Ω—Ç–µ—Ä–Ω–µ—Ç –∏–ª–∏ —Å–≤–æ–∏ –¥–æ–≥–∞–¥–∫–∏.
        - –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ù–ï–¢ ‚Äî –ø—Ä—è–º–æ —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏: 
          "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ—Ç —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."
        - –ù–ï –ø–æ–¥–º–µ–Ω—è–π –æ–¥–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ/–ø–ª–∞—Ç—É –¥—Ä—É–≥–∏–º.
        - –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É, –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
        - –ï—Å–ª–∏ –µ—Å—Ç—å —Ç–∞–±–ª–∏—Ü–∞ –∏–ª–∏ —è–≤–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è ‚Äî –ø—Ä–∏–≤–µ–¥–∏ –∏—Ö —Å–ª–æ–≤–∞–º–∏ –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ.

        –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:
        - –ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô –∏ –ù–ï –î–û–ì–ê–î–´–í–ê–ô–°–Ø.
        - –õ—É—á—à–µ —á–µ—Å—Ç–Ω–æ –æ—Ç–≤–µ—Ç—å "–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ <—Ç–µ—Ä–º–∏–Ω> –≤ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö –Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞", —á–µ–º –Ω–∞–ø–∏—Å–∞—Ç—å —á—É—à—å."""

        prompt_parts = []

        if history_text:
            prompt_parts.append(f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{history_text}\n")

        prompt_parts.append(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:\n{context}\n")
        prompt_parts.append(f"–¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{user_query}\n")
        prompt_parts.append("–û—Ç–≤–µ—Ç:")

        prompt = "\n".join(prompt_parts)

        answer = self.ollama.generate(prompt, system_prompt=system_prompt)

        # 5. –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = [
            {
                "file": doc["file"],
                "page": doc["page"],
                "score": round(doc["score"], 3),
            }
            for doc in documents
        ]

        return {
            "answer": answer,
            "sources": sources,
            "relevance": relevance_percent
        }

    def generate_clarification_questions(self, user_query: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        system_prompt = """–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏.
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å.
–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π 3-5 –∫–æ—Ä–æ—Ç–∫–∏—Ö —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ (–∫–∞–∂–¥—ã–π –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ).
–í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –û–ß–ï–ù–¨ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ (–º–∞–∫—Å–∏–º—É–º 5-7 —Å–ª–æ–≤)."""

        prompt = f"""–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_query}

–£—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:"""

        try:
            response = self.ollama.generate(prompt, system_prompt=system_prompt)

            # –ü–∞—Ä—Å–∏–º –≤–æ–ø—Ä–æ—Å—ã
            questions = [q.strip() for q in response.split('\n') if q.strip()]
            questions = [q.lstrip('0123456789.-) ') for q in questions]  # –£–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é

            return questions[:5]  # –ú–∞–∫—Å–∏–º—É–º 5 –≤–æ–ø—Ä–æ—Å–æ–≤

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {repr(e)}")
            return []

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        vector_stats = self.vector_store.get_stats()

        return {
            "indexed_files_count": len(self.indexed_files),
            "indexed_files_list": self.indexed_files,
            "total_documents": vector_stats.get("total_documents", 0),
            "vector_size": vector_stats.get("vector_size", 0),
            "embedding_model": vector_stats.get("model", "unknown"),
        }

    def test_connection(self) -> Dict[str, str]:
        """–¢–µ—Å—Ç –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        results = []

        # 1. –¢–µ—Å—Ç Ollama
        try:
            ok = self.ollama.test_connection()
            if ok:
                results.append(f"‚úÖ Ollama: –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, –º–æ–¥–µ–ª—å {self.ollama.model} –¥–æ—Å—Ç—É–ø–Ω–∞.")
            else:
                results.append(f"‚ùå Ollama: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ {self.ollama.model}.")
        except Exception as e:
            results.append(f"‚ùå Ollama: –æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {repr(e)}")

        # 2. –¢–µ—Å—Ç Qdrant
        try:
            qdrant_test = self.vector_store.test_connection()
            if isinstance(qdrant_test, dict) and "message" in qdrant_test:
                results.append(qdrant_test["message"])
            else:
                results.append(f"‚ÑπÔ∏è Qdrant: {qdrant_test}")
        except Exception as e:
            results.append(f"‚ùå Qdrant: –æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {repr(e)}")

        return {"message": "\n\n".join(results)}