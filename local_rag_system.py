import logging
import json
import time
import psutil
from pathlib import Path
from typing import List, Dict

from tqdm import tqdm

from local_config import (
    DOCUMENTS_FOLDER,
    TOP_K_RESULTS,
    OLLAMA_BASE_URL,
    OLLAMA_MODEL,
    OLLAMA_TEMPERATURE,
)
from local_document_processor import DocumentProcessor
from local_vector_store import VectorStore
from local_ollama_client import OllamaClient

logger = logging.getLogger(__name__)


class RAGSystem:
    """RAG —Å–∏—Å—Ç–µ–º–∞: –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è + –ø–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤"""

    def __init__(self):
        self.document_processor = DocumentProcessor()
        self.vector_store = VectorStore()

        self.ollama = OllamaClient(
            base_url=OLLAMA_BASE_URL,
            model=OLLAMA_MODEL,
            temperature=OLLAMA_TEMPERATURE,
        )

        self.indexed_files_path = Path("indexed_files.json")
        self.indexed_files = self._load_indexed_files()

        self._indexing = False
        self._stop_indexing = False

        logger.info("‚úÖ RAGSystem –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    # ==============================
    # –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –£–¢–ò–õ–ò–¢–´
    # ==============================

    def _load_indexed_files(self) -> List[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        if self.indexed_files_path.exists():
            try:
                with open(self.indexed_files_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                    return data.get("indexed_files", [])
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ indexed_files.json: {repr(e)}")
        return []

    def _save_indexed_files(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        try:
            with open(self.indexed_files_path, "w", encoding="utf-8") as f:
                json.dump(
                    {"indexed_files": self.indexed_files},
                    f,
                    ensure_ascii=False,
                    indent=2,
                )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è indexed_files.json: {repr(e)}")

    @staticmethod
    def _is_general_chat(query: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ–±—â–∏–π/–±–æ–ª—Ç–æ–≤–æ–π –≤–æ–ø—Ä–æ—Å, –Ω–µ –ø—Ä–æ —Ç–µ—Ö–Ω–∏–∫—É"""
        text = query.lower()

        general_keywords = [
            "–∫–∞–∫ —Ç–µ–±—è –∑–æ–≤—É—Ç",
            "–∫–∞–∫ —Ç–≤–æ–µ –∏–º—è",
            "–∫–∞–∫ —Ç–≤–æ—ë –∏–º—è",
            "—á—Ç–æ —Ç—ã —É–º–µ–µ—à—å",
            "–∫—Ç–æ —Ç—ã",
            "—Ä–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç",
            "–∞–Ω–µ–∫–¥–æ—Ç",
            "—à—É—Ç–∫—É —Ä–∞—Å—Å–∫–∞–∂–∏",
            "—à—É—Ç–∫–∞",
            "–∫—Ç–æ —Ç–µ–±—è —Å–æ–∑–¥–∞–ª",
            "—á—Ç–æ —Ç—ã —Ç–∞–∫–æ–µ",
            "—á—Ç–æ —Ç—ã –º–æ–∂–µ—à—å",
        ]

        return any(kw in text for kw in general_keywords)

    @staticmethod
    def _is_elevator_related(query: str) -> bool:
        """
        –ü—Ä–∏–º–∏—Ç–∏–≤–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –∑–∞–ø—Ä–æ—Å –ø—Ä–æ –ª–∏—Ñ—Ç—ã –∏–ª–∏ –Ω–µ—Ç.
        –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –µ—Å–ª–∏ –µ—Å—Ç—å "–ø–ª–∞—Ç–∞", "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞", "–æ—à–∏–±–∫–∞" –∏ —Ç.–ø. ‚Äî —ç—Ç–æ –ª–∏—Ñ—Ç—ã.
        """
        text = query.lower()

        elevator_keywords = [
            "–ª–∏—Ñ—Ç",
            "–∫–∞–±–∏–Ω–∞",
            "—à–∞—Ö—Ç–∞",
            "–æ–≥—Ä–∞–Ω–∏—á–∏—Ç–µ–ª—å —Å–∫–æ—Ä–æ—Å—Ç–∏",
            "—Å—Ç–∞–Ω—Ü–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è",
            "—Å—Ç–∞–Ω—Ü–∏—è —É–ø—Ä.",
            "—Å—É-–∫",
            "—Å—É–∫-",
            "–º—Å-1",
            "ms-1",
            "ms1",
            "–ª–µ–±–µ–¥–∫–∞",
            "–ª–µ–±–∏ÃÜ–¥–∫–∞",
            "door",
            "elevator",
        ]

        tech_action_keywords = [
            "–ø–ª–∞—Ç–∞",
            "board",
            "–∫–æ–Ω—Ç—Ä–æ–ª–ª–µ—Ä",
            "–±–ª–æ–∫",
            "–º–æ–¥—É–ª—å",
            "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞",
            "–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å",
            "–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è",
            "–ø–∞—Ä–∞–º–µ—Ç—Ä",
            "–ø–∞—Ä–∞–º–µ—Ç—Ä—ã",
            "–æ—à–∏–±–∫–∞",
            "fault",
            "alarm",
            "–∫–∞–∫ —Å–¥–µ–ª–∞—Ç—å",
            "–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å",
            "–∫–∞–∫ –ø–æ–¥–∫–ª—é—á–∏—Ç—å",
            "–ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ",
            "–¥–∏–ø-–ø–µ—Ä–µ–∫–ª—é—á–∞—Ç–µ–ª—å",
            "dip",
            "jumper",
            "–ø–µ—Ä–µ–º—ã—á–∫–∞",
        ]

        for kw in elevator_keywords + tech_action_keywords:
            if kw in text:
                return True

        return False

    # ==============================
    # –ò–ù–î–ï–ö–°–ê–¶–ò–Ø
    # ==============================

    def index_documents(self, continue_indexing: bool = True):
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ documents/
        """
        if self._indexing:
            logger.warning("‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É–∂–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è")
            return

        self._indexing = True
        self._stop_indexing = False

        process_start = time.time()
        process = psutil.Process()

        try:
            if not continue_indexing:
                logger.info("üîÑ –ù–ê–ß–ò–ù–ê–Æ –ü–û–õ–ù–£–Æ –ü–ï–†–ï–ò–ù–î–ï–ö–°–ê–¶–ò–Æ (–æ—á–∏—Å—Ç–∫–∞ –ë–î)...")
                self.vector_store.clear_collection()
                self.indexed_files = []
                logger.info("‚úÖ –í–µ–∫—Ç–æ—Ä–Ω–∞—è –ë–î –æ—á–∏—â–µ–Ω–∞")

            files = list(DOCUMENTS_FOLDER.glob("*.pdf")) + list(
                DOCUMENTS_FOLDER.glob("*.docx")
            )

            if not files:
                logger.warning(f"‚ö†Ô∏è –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –≤ –ø–∞–ø–∫–µ {DOCUMENTS_FOLDER}")
                return

            if continue_indexing:
                files_before = len(files)
                files = [f for f in files if f.name not in self.indexed_files]
                files_after = len(files)
                logger.info(
                    f"üìä –§–∞–π–ª–æ–≤ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {files_before}, –ø–æ—Å–ª–µ: {files_after}"
                )

            if not files:
                logger.info("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω—ã")
                return

            logger.info(f"üìö –ù–ê–ô–î–ï–ù–û –§–ê–ô–õ–û–í –î–õ–Ø –ò–ù–î–ï–ö–°–ê–¶–ò–ò: {len(files)}")
            logger.info(f"üìä –ù–∞—á–∏–Ω–∞—é –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é (continue={continue_indexing})")
            logger.info(
                f"üíæ –ü–∞–º—è—Ç—å –≤ –Ω–∞—á–∞–ª–µ: {process.memory_info().rss / 1024 / 1024:.1f}MB"
            )

            total_fragments = 0
            total_files_processed = 0
            failed_files = []

            for file_idx, file_path in enumerate(
                tqdm(files, desc="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è", unit="—Ñ–∞–π–ª"), 1
            ):
                if self._stop_indexing:
                    logger.info("üõë –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
                    break

                logger.info("\n" + "=" * 80)
                logger.info(f"üìÅ –§–ê–ô–õ {file_idx}/{len(files)}: {file_path.name}")
                logger.info("=" * 80)
                file_start = time.time()

                try:
                    logger.info("üìÑ –ù–∞—á–∏–Ω–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞...")
                    fragments = self.document_processor.process_file(file_path)

                    if not fragments:
                        logger.warning(
                            f"‚ö†Ô∏è –§–∞–π–ª {file_path.name} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–∞"
                        )
                        failed_files.append((file_path.name, "–Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞"))
                        continue

                    logger.info(
                        f"üì§ –ó–∞–≥—Ä—É–∑–∫–∞ {len(fragments)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î..."
                    )
                    db_start = time.time()
                    self.vector_store.add_documents(fragments)
                    db_time = time.time() - db_start

                    self.indexed_files.append(file_path.name)
                    self._save_indexed_files()

                    total_fragments += len(fragments)
                    total_files_processed += 1

                    file_time = time.time() - file_start
                    memory_usage = process.memory_info().rss / 1024 / 1024

                    logger.info(f"‚úÖ –§–ê–ô–õ {file_path.name} –£–°–ü–ï–®–ù–û –ü–†–û–ò–ù–î–ï–ö–°–ò–†–û–í–ê–ù:")
                    logger.info(f"   üìä –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(fragments)}")
                    logger.info(
                        f"   ‚è±Ô∏è –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞: {file_time:.1f}—Å"
                    )
                    logger.info(
                        f"   ‚è±Ô∏è –í—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –ë–î: {db_time:.1f}—Å"
                    )
                    logger.info(f"   üíæ –ü–∞–º—è—Ç—å: {memory_usage:.1f}MB")
                    if file_time > 0:
                        logger.info(
                            f"   üìà –°–∫–æ—Ä–æ—Å—Ç—å: {len(fragments) / file_time:.1f} —Ñ—Ä–∞–≥–º/—Å–µ–∫"
                        )

                except Exception as e:
                    logger.error(
                        f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ü–†–ò –û–ë–†–ê–ë–û–¢–ö–ï –§–ê–ô–õ–ê {file_path.name}: {repr(e)}"
                    )
                    failed_files.append((file_path.name, str(e)))
                    continue

            total_time = time.time() - process_start
            final_memory = process.memory_info().rss / 1024 / 1024

            logger.info("\n" + "=" * 80)
            logger.info("üéâ –ò–ù–î–ï–ö–°–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
            logger.info("=" * 80)
            logger.info("üìä –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
            logger.info(f"   üìÅ –í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {len(files)}")
            logger.info(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {total_files_processed}")
            logger.info(f"   ‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(failed_files)}")
            logger.info(f"   üìÑ –í—Å–µ–≥–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {total_fragments}")
            logger.info(f"   ‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}—Å")
            if total_time > 0:
                logger.info(
                    f"   üìà –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_fragments / total_time:.1f} —Ñ—Ä–∞–≥–º/—Å–µ–∫"
                )
            logger.info(f"   üíæ –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ: {final_memory:.1f}MB")

            if total_files_processed > 0:
                logger.info(
                    f"   üìä –°—Ä–µ–¥–Ω–µ–µ –Ω–∞ —Ñ–∞–π–ª: {total_fragments / total_files_processed:.1f} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤"
                )

            if failed_files:
                logger.warning("\n‚ö†Ô∏è –ù–ï –£–î–ê–õ–û–°–¨ –û–ë–†–ê–ë–û–¢–ê–¢–¨ –§–ê–ô–õ–´:")
                for file_name, error in failed_files:
                    logger.warning(f"   ‚Ä¢ {file_name}: {error}")

        except Exception as e:
            logger.error(
                f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –ü–†–ò –ò–ù–î–ï–ö–°–ê–¶–ò–ò: {repr(e)}"
            )
            raise

        finally:
            self._indexing = False
            self._stop_indexing = False

    def is_indexing(self) -> bool:
        return self._indexing

    def stop_indexing_process(self):
        self._stop_indexing = True

    # ==============================
    # –û–°–ù–û–í–ù–û–ô –ó–ê–ü–†–û–° (–ë–ï–ó –ò–°–¢–û–†–ò–ò)
    # ==============================

    def query(self, user_query: str, top_k: int = TOP_K_RESULTS) -> Dict:
        """
        –ü–æ–∏—Å–∫ + –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        """
        logger.info(f"üí¨ –ó–ê–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {user_query}")

        # 0. –û–±—â–∏–π –±–æ–ª—Ç–æ–≤–æ–π –≤–æ–ø—Ä–æ—Å ‚Äî –æ—Ç–≤–µ—á–∞–µ–º –±–µ–∑ RAG
        if self._is_general_chat(user_query):
            logger.info("üí¨ –û–±–Ω–∞—Ä—É–∂–µ–Ω –æ–±—â–∏–π –≤–æ–ø—Ä–æ—Å (–Ω–µ –ø—Ä–æ –ª–∏—Ñ—Ç—ã) ‚Äî –æ—Ç–≤–µ—á–∞–µ–º –±–µ–∑ RAG")
            answer = self.ollama.generate(
                prompt=f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–æ—Å–∏–ª: {user_query}\n–û—Ç–≤–µ—Ç—å –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.",
                system_prompt="–¢—ã –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ú–æ–∂–Ω–æ —á—É—Ç—å –ø–æ—à—É—Ç–∏—Ç—å, –Ω–æ –±–µ–∑ –∂—ë—Å—Ç–∫–æ–≥–æ –º–∞—Ç–∞.",
                max_tokens=256,
            )
            return {"answer": answer, "sources": [], "relevance": 0.0}

        # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–µ –ª–∏ –Ω–∞ –ª–∏—Ñ—Ç–æ–≤—É—é/—Ç–µ—Ö–Ω–∏—á. —Ç–µ–º—É
        is_elevator = self._is_elevator_related(user_query)
        if not is_elevator:
            logger.info(
                "‚ÑπÔ∏è –ó–∞–ø—Ä–æ—Å –Ω–µ –ø–æ—Ö–æ–∂ –Ω–∞ –ª–∏—Ñ—Ç–æ–≤—É—é —Ç–µ–º–∞—Ç–∏–∫—É. –û—Ç–≤–µ—á–∞–µ–º –∫–∞–∫ –æ–±—â–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –±–µ–∑ RAG."
            )
            answer = self.ollama.generate(
                prompt=f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–æ—Å–∏–ª: {user_query}\n–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.",
                system_prompt="–¢—ã –æ–±—â–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª—é–±—ã–µ —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è.",
                max_tokens=256,
            )
            return {"answer": answer, "sources": [], "relevance": 0.0}

        # 2. –í–æ–ø—Ä–æ—Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –ø–æ –ª–∏—Ñ—Ç–∞–º ‚Äî –≤–∫–ª—é—á–∞–µ–º RAG
        logger.info("üîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
        documents = self.vector_store.search(user_query, top_k=top_k)

        if not documents:
            logger.warning(
                f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {user_query}"
            )
            return {
                "answer": "‚ùå –í –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏—á–µ–≥–æ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.",
                "sources": [],
                "relevance": 0.0,
            }

        avg_score = sum(doc.get("score", 0) for doc in documents) / len(documents)
        relevance_percent = avg_score * 100

        context_parts = []
        for idx, doc in enumerate(documents, start=1):
            context_parts.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {idx}: {doc['file']}, —Å—Ç—Ä. {doc['page']}]\n{doc['content']}"
            )

        context = "\n\n---\n\n".join(context_parts)

        system_prompt = """–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π –ø–æ –ª–∏—Ñ—Ç–∞–º –∏ –ª–∏—Ñ—Ç–æ–≤–æ–º—É –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—é.

–ü–†–ê–í–ò–õ–ê –î–õ–Ø –õ–ò–§–¢–û–í–û–ô –¢–ï–ú–´:
- –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –ø–æ –ª–∏—Ñ—Ç–∞–º, –ø–ª–∞—Ç–∞–º, –æ—à–∏–±–∫–∞–º, –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
- –ï—Å–ª–∏ –Ω—É–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —è–≤–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π –µ—ë –∏ –¥–∞–≤–∞–π —á—ë—Ç–∫–∏–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç.
- –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ù–ï–¢ ‚Äî –ø—Ä—è–º–æ —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏:
  "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ—Ç —Ç–æ—á–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."
- –ù–ï –ø–æ–¥–º–µ–Ω—è–π –æ–¥–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ/–ø–ª–∞—Ç—É –¥—Ä—É–≥–∏–º.
- –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –∏ –ø–æ –¥–µ–ª—É, –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
- –ï—Å–ª–∏ –µ—Å—Ç—å —è–≤–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏/—Ç–∞–±–ª–∏—Ü—ã/–ø–µ—Ä–µ—á–Ω–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ ‚Äî –ø–µ—Ä–µ–¥–∞–≤–∞–π –∏—Ö —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ (—Å–ª–æ–≤–∞–º–∏ –∏–ª–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ).

–û–ë–©–ï–ï:
- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–µ –ø—Ä–æ –ª–∏—Ñ—Ç—ã (–æ–±—â–∏–π, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏–π, –ø—Ä–æ –ò–ò, –∂–∏–∑–Ω—å –∏ —Ç.–ø.) ‚Äî –º–æ–∂–Ω–æ –æ—Ç–≤–µ—á–∞—Ç—å –∫–∞–∫ –æ–±—ã—á–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –Ω–µ —Å—Å—ã–ª–∞—è—Å—å –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
- –ù–æ –≤ —Ç–µ–∫—É—â–µ–º —Ä–µ–∂–∏–º–µ –º—ã –∏—Å—Ö–æ–¥–∏–º –∏–∑ —Ç–æ–≥–æ, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ –ª–∏—Ñ—Ç–∞–º.

–ï–°–õ–ò –í–ê–ú –ù–ï –•–í–ê–¢–ê–ï–¢ –î–ê–ù–ù–´–•:
- –ï—Å–ª–∏ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –≤–∏–¥–Ω–æ, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–∞–ª–æ –∏–ª–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –¥–µ—Ç–∞–ª–µ–π (–Ω–µ—Ç –º–æ–¥–µ–ª–∏, –ø–ª–∞—Ç—ã, —Ä–µ–∂–∏–º–∞ –∏ —Ç.–ø.) ‚Äî –º–æ–∂–Ω–æ –≤ –∫–æ–Ω—Ü–µ –æ—Ç–≤–µ—Ç–∞ —è–≤–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —É—Ç–æ—á–Ω–∏—Ç—å: –∫–∞–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã/–º–æ–¥–µ–ª—å/–ø–ª–∞—Ç—É –æ–Ω –∏–º–µ–µ—Ç –≤ –≤–∏–¥—É."""

        prompt = f"""–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:

{context}

–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
{user_query}

–û—Ç–≤–µ—Ç:"""

        logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...")
        answer = self.ollama.generate(
            prompt, system_prompt=system_prompt, max_tokens=800
        )

        sources = [
            {
                "file": doc["file"],
                "page": doc["page"],
                "score": round(doc["score"], 3),
            }
            for doc in documents
        ]

        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance_percent:.1f}%")

        return {"answer": answer, "sources": sources, "relevance": relevance_percent}

    # ==============================
    # –ó–ê–ü–†–û–° –° –ò–°–¢–û–†–ò–ï–ô
    # ==============================

    def query_with_history(
        self, history: List[Dict], user_query: str, top_k: int = TOP_K_RESULTS
    ) -> Dict:
        logger.info(f"üí¨ –ó–ê–ü–†–û–° –° –ò–°–¢–û–†–ò–ï–ô: {user_query}")
        logger.info(f"üìä –†–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏: {len(history)} —Å–æ–æ–±—â–µ–Ω–∏–π")

        # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ –∂–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ (–æ–±—â–∏–π/–ª–∏—Ñ—Ç–æ–≤—ã–π)
        if self._is_general_chat(user_query) and not self._is_elevator_related(
            user_query
        ):
            logger.info(
                "üí¨ –û–±—â–∏–π –≤–æ–ø—Ä–æ—Å —Å –∏—Å—Ç–æ—Ä–∏–µ–π (–Ω–æ –Ω–µ –ª–∏—Ñ—Ç–æ–≤—ã–π) ‚Äî –æ—Ç–≤–µ—á–∞–µ–º –±–µ–∑ RAG"
            )
            history_text = ""
            if history:
                lines = []
                for msg in history[-10:]:
                    role = msg.get("role", "user")
                    content = (msg.get("content") or "").strip()
                    if not content:
                        continue
                    prefix = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:" if role == "user" else "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
                    lines.append(f"{prefix} {content}")
                if lines:
                    history_text = "\n".join(lines)

            prompt = "–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n" + history_text + "\n\n" if history_text else ""
            prompt += f"–¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{user_query}\n\n–û—Ç–≤–µ—Ç—å –¥—Ä—É–∂–µ–ª—é–±–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."

            answer = self.ollama.generate(
                prompt,
                system_prompt="–¢—ã —Ä–∞–∑–≥–æ–≤–æ—Ä–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –º–æ–∂–Ω–æ –Ω–µ–º–Ω–æ–≥–æ —à—É—Ç–∏—Ç—å.",
                max_tokens=256,
            )
            return {"answer": answer, "sources": [], "relevance": 0.0}

        # –õ–∏—Ñ—Ç–æ–≤–∞—è —Ç–µ–º–∞ ‚Äî RAG
        logger.info("üîç –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Å –∏—Å—Ç–æ—Ä–∏–µ–π)...")
        documents = self.vector_store.search(user_query, top_k=top_k)

        if not documents:
            logger.warning(
                f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {user_query}"
            )
            return {
                "answer": "‚ùå –í –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏—á–µ–≥–æ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É.",
                "sources": [],
                "relevance": 0.0,
            }

        avg_score = sum(doc.get("score", 0) for doc in documents) / len(documents)
        relevance_percent = avg_score * 100

        context_parts = []
        for idx, doc in enumerate(documents, start=1):
            context_parts.append(
                f"[–ò—Å—Ç–æ—á–Ω–∏–∫ {idx}: {doc['file']}, —Å—Ç—Ä. {doc['page']}]\n{doc['content']}"
            )
        context = "\n\n---\n\n".join(context_parts)

        history_text = ""
        if history:
            lines = []
            for msg in history[-10:]:
                role = msg.get("role", "user")
                content = (msg.get("content") or "").strip()
                if not content:
                    continue
                prefix = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å:" if role == "user" else "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
                lines.append(f"{prefix} {content}")
            if lines:
                history_text = "\n".join(lines)
                logger.info(
                    f"üìú –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏—Å—Ç–æ—Ä–∏—è –∏–∑ {len(lines)} —Å–æ–æ–±—â–µ–Ω–∏–π"
                )

        system_prompt = """–¢—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–µ–π –ø–æ –ª–∏—Ñ—Ç–∞–º –∏ –ª–∏—Ñ—Ç–æ–≤–æ–º—É –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—é.

–ü–†–ê–í–ò–õ–ê:
- –û—Å–Ω–æ–≤—ã–≤–∞–π—Å—è –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∏ –Ω–µ–¥–∞–≤–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏ –¥–∏–∞–ª–æ–≥–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–µ–ø–ª–∏–∫–∏).
- –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –¥–∞–Ω–Ω—ã—Ö –ø–æ –ª–∏—Ñ—Ç–æ–≤–æ–π —Ç–µ–º–∞—Ç–∏–∫–µ, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ.
- –õ—É—á—à–µ –ø—Ä—è–º–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ –¥–∞–Ω–Ω—ã—Ö –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–µ—Ç, —á–µ–º —Ñ–∞–Ω—Ç–∞–∑–∏—Ä–æ–≤–∞—Ç—å.
- –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É, –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
- –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —è–≤–Ω–æ –º–∞–ª–æ ‚Äî –º–æ–∂–µ—à—å –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é —É—Ç–æ—á–Ω–∏—Ç—å –º–æ–¥–µ–ª—å, –ø–ª–∞—Ç—É, —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã, –∫–æ–¥ –æ—à–∏–±–∫–∏ –∏ —Ç.–ø."""

        prompt_parts = []
        if history_text:
            prompt_parts.append(f"–ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞:\n{history_text}\n")
        prompt_parts.append(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏:\n{context}\n")
        prompt_parts.append(f"–¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n{user_query}\n")
        prompt_parts.append("–û—Ç–≤–µ—Ç:")

        prompt = "\n".join(prompt_parts)

        logger.info("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM —Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏...")
        answer = self.ollama.generate(
            prompt, system_prompt=system_prompt, max_tokens=800
        )

        sources = [
            {
                "file": doc["file"],
                "page": doc["page"],
                "score": round(doc["score"], 3),
            }
            for doc in documents
        ]

        logger.info(f"‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance_percent:.1f}%")

        return {"answer": answer, "sources": sources, "relevance": relevance_percent}

    # ==============================
    # –£–¢–û–ß–ù–Ø–Æ–©–ò–ï –í–û–ü–†–û–°–´ + –°–¢–ê–¢–ò–°–¢–ò–ö–ê
    # ==============================

    def generate_clarification_questions(self, user_query: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ (–ø–æ–∫–∞ –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —è–≤–Ω–æ, –±–µ–∑ –∞–≤—Ç–æ–ª–æ–æ–ø–∞)"""
        system_prompt = """–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–∏.
–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞–ª –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω—ã–π –≤–æ–ø—Ä–æ—Å.
–°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –∫–æ—Ä–æ—Ç–∫–∏–µ —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã (–∫–∞–∂–¥—ã–π –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ).
–í–æ–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –û–ß–ï–ù–¨ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ (–º–∞–∫—Å–∏–º—É–º 5-7 —Å–ª–æ–≤).
–ì–µ–Ω–µ—Ä–∏—Ä—É–π —Å—Ç–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤, —Å–∫–æ–ª—å–∫–æ —Å—á–∏—Ç–∞–µ—à—å –Ω—É–∂–Ω—ã–º –¥–ª—è —É—Ç–æ—á–Ω–µ–Ω–∏—è."""

        prompt = f"""–í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {user_query}

–£—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã:"""

        try:
            logger.info(
                f"‚ùì –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è: {user_query}"
            )
            response = self.ollama.generate(
                prompt, system_prompt=system_prompt, max_tokens=256
            )

            questions = [
                q.strip() for q in response.split("\n") if q.strip()
            ]
            questions = [
                q.lstrip("0123456789.-) ") for q in questions
            ]  # —É–±–∏—Ä–∞–µ–º –Ω—É–º–µ—Ä–∞—Ü–∏—é
            questions = [
                q for q in questions if q and len(q.split()) <= 10
            ]

            logger.info(
                f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(questions)} —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"
            )
            return questions

        except Exception as e:
            logger.error(
                f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —É—Ç–æ—á–Ω—è—é—â–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤: {repr(e)}"
            )
            return []

    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã"""
        vector_stats = self.vector_store.get_stats()

        return {
            "indexed_files_count": len(self.indexed_files),
            "indexed_files_list": self.indexed_files,
            "total_documents": vector_stats.get("total_documents", 0),
            "vector_size": vector_stats.get("vector_size", 0),
            "embedding_model": vector_stats.get("model", "unknown"),
        }

    def test_connection(self) -> Dict[str, str]:
        """–¢–µ—Å—Ç –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        results = []

        try:
            ok = self.ollama.test_connection()
            if ok:
                results.append(
                    f"‚úÖ Ollama: –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ, –º–æ–¥–µ–ª—å {self.ollama.model} –¥–æ—Å—Ç—É–ø–Ω–∞."
                )
            else:
                results.append(
                    f"‚ùå Ollama: –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ {self.ollama.model}."
                )
        except Exception as e:
            results.append(f"‚ùå Ollama: –æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {repr(e)}")

        try:
            qdrant_test = self.vector_store.test_connection()
            if isinstance(qdrant_test, dict) and "message" in qdrant_test:
                results.append(qdrant_test["message"])
            else:
                results.append(f"‚ÑπÔ∏è Qdrant: {qdrant_test}")
        except Exception as e:
            results.append(f"‚ùå Qdrant: –æ—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {repr(e)}")

        return {"message": "\n\n".join(results)}